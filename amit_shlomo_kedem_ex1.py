# -*- coding: utf-8 -*-
"""amit_Shlomo_kedem_ex1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cri4ja1GZa3pjx-Ceq_HlmGtTbl0_syC

```
# This is formatted as code
```

Student 1: name: Amit Shlomo Kedem , i.d.: 315216663 , github:
Student 2: name: , i.d.: , github:
Student 3: name: , i.d.: , github:

1. Load breast cancer dataset (**structured data**)

For more details about the data: https://scikit-learn.org/1.5/modules/generated/sklearn.datasets.load_breast_cancer.html
"""

'''
[Amit] in this part we load the module we want to research, in this project - breast cancer.
the data is being imported from sklearn.datasets (it contains various resarch values (fires,brain cancer, false pregnancy))
'''
from sklearn.datasets import load_breast_cancer
my_data = load_breast_cancer()

"""2. Split **my_data** to train and test:

- Define X_train, X_test, Y_train, Y_test
- Choose **test_size** for splitting **my_data**
- Use **train_test_split** (for details: https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.train_test_split.html)
"""

from sklearn.model_selection import train_test_split

X = my_data.data
y = my_data.target
'''
[Amit] In this part we divide the data we recieved in the import section, we always divide it to
x train,test , y train,test. in the () we decide the test size (the most common is 0.2 for testing, and 0.8 for training)
random state is used so we will recieve the same data structure (testing <-> training every time we run the notebook.
we can choose every number but it's important to use the same number every time.
'''
X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size=0.2,random_state=42)

"""3. Libraries"""

'''
[Amit] This is the imports that we need to be able to train the module -
we do not touch that part and it's forbidden to change it (Per Igor's request).
'''
!pip install mlflow
!pip install mlflow scikit-learn

import mlflow
import mlflow.sklearn
from mlflow import log_param, log_metric

from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

import itertools
import pandas as pd

"""4. Define MLFlow experiment"""

'''
[Amit] MLFlow Is a tool to monitor and track the results of our training sessions.
you can see how it works in code block 5.1
'''
EXPERIMENT_NAME = "trees_hyperparam"
mlflow.set_experiment(EXPERIMENT_NAME)
# MLFlow details: https://mlflow.org/docs/latest/ml/tracking/

"""5. Train **model_decision_tree**

- Library: sklearn.tree.DecisionTreeClassifier
- Data: X_train, Y_train
- **Essential**: explore and optimize DecisionTreeClassifier options   
"""

from sklearn.tree import DecisionTreeClassifier

'''
[Amit] This part defines the parameters we want to check.
since DecisionTreeClassifier expects variavbles (or None). we decided to give him 3 variables, based on params_x_list
It's important to remember :
x = Features about the current subject
Y = binary assunption (0/1) in our case - breast cancer 0 - Not a tumor, 1 = Real tumor
in this project - the params we currently use are - max_depth (param_1), min_samples_split (param_2) and criterion (param_3)
(took the most common from the docs)
'''
#TODO: need to check which params Igor taught us and use them if these are not the ones.
'''
[Amit]
param_1_list : here - we decide the depth of the tree that we want to check. (None == Unlimited)
param_2_list : here we decide what is the mininum number of samples that a node must have before splitting into 2 childs.
               this param must be > 1 because you need at least 2 samples to be able to split them into 2 groups.
param_3_list : for criterion - there are only 2 options.
               and we chose to use both in order to get as many results as possible (the more the better :) )
               * Gini -  measures how mixed the classes are inside a node.
               * measures uncertainty in a node — coming from information theory.
'''

param_1_list =[1,2,3,4,5,6,7,8,9,None]
param_2_list = [2,3,4,5,6,7,8,9]
param_3_list = ["gini","entropy"]

'''
[Amit] In this part - we create param_grid - which generates all possible combinations of parameters
so if for example - if my data is :
x = 1,2,3
y = 3,4,5
z = 6,7,8

i will get
x1,y1,z1 = 1,3,6
x2,y2,z2 = 2,4,7
x3,y3,z3 = 3,5,8
and so on, untill all is covered.
'''
param_grid = list(itertools.product(param_1_list, param_2_list, param_3_list))

'''
[Amit] here - we actually start to train the model !
each time we take a differnt param from the grid, and iterate the entire grid ^
'''
for param_1, param_2, param_3 in param_grid:
    with mlflow.start_run():

        '''
        [Amit] this part is perhaps the most imporant in the training session - Because altough we are able to train our model without it -
        if we dont log the params - we won't be able to tell which combination from the grid produced the best score !
        '''
        # Log parameters
        mlflow.log_param("model_type", "DecisionTree")
        mlflow.log_param("param_1", param_1)
        mlflow.log_param("param_2", param_2)
        mlflow.log_param("param_3", param_3)
        '''
        [Amit] Here - the training begin !
        the function DecisionTreeClassifier has a lot of options, and it is up to us to decide what to give it,
        since we took max_depth, min_samples_split and criterion - we will give it to the function and and each time provide a param from the loop.
        '''
        # Train the model
        d_tree = DecisionTreeClassifier(max_depth=param_1, min_samples_split=param_2, criterion=param_3)
        d_tree.fit(X_train, Y_train)
        pred = d_tree.predict(X_test)
        '''
        [Amit] this part calculate the "scores" for the current iteration (in terms of accuracy, precision and recall)
        * acc - Out of ALL predictions, how many did the model get right?
        * pre - How many “positive” predictions were actually correct?
        * rec - How many actual positive cases did we successfully catch?
        * f1 -  Balance between precision and recall

        '''
        acc  = accuracy_score(Y_test, pred)
        pre = precision_score(Y_test, pred)
        rec  = recall_score(Y_test, pred)
        f1   = f1_score(Y_test, pred)


        # run test prediction and calculate metrics:
        # accuracy_score
        # precision_score
        # recall_score
        # f1_score

        '''
        [Amit] Here - we simply log the results to the mlflow.
        we do it in order to observe and look at the results the training session produced.
        we can see how it is logged in the next code block :)
        '''
        # Log metrics
        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("precision_score", pre)
        mlflow.log_metric("recall_score", rec)
        mlflow.log_metric("f1_score", f1)

"""5.1. [Extra Step] View results from **DecisionTreeClassifier**


TODO: Remove this before submitting
"""

'''
[Amit] I added this part so we will be able to see the results we trained in the last Code block.
I decided to filter the results based on what I want, for example:
I don't want to see the failed results (simply because the combination of the params failed), and I want to see only the results with the best scores
I used pandas to filter the scores and sort them based on the best scores.
(the best 10 are picked based on f1 (since it's the correlation between precision and recall)

Tip :
      * if you want to see all results , simply comment out this block and instead put :
        there should be 160 entries.

      * If we did everything correctly - the best results **so far** are :
        param1 = 3
        param2 = 7 or 6
        param3 = entropy


mlflow.search_runs(experiment_names=["trees_hyperparam"])
'''

df = mlflow.search_runs(experiment_names=["trees_hyperparam"])
df_passed = df[df["status"] == "FINISHED"]
df_passed_DecisionTree = df_passed[df_passed["params.model_type"]=="DecisionTree"]
df_top10 = df_passed_DecisionTree.sort_values("metrics.f1_score", ascending=False).head(10)
df_top10

"""6. Train model_random_forest
- Library: sklearn.ensemble.RandomForestClassifier
- Data: X_train, Y_train
- **Essential**: explore and optimize RandomForestClassifier options
"""

from sklearn.ensemble import RandomForestClassifier

'''
[Amit] Random forest works technically the same as decisionTree in terms of params.
it can recieve a lot of different params, I decided to take the most common 3 (from the docs)
* param_1_list = n_estimators : Number of trees in the forest
* param_2_list = max_depth : depth limit in each tree where None = Infinity
* param_3_list = max_features : How many features each tree can use per split.

'''
#TODO: Check if Igor actually taught these params - if not, change to what we taught
param_1_list = [1, 5, 10, 50, 100, 200, 500, 1000]
param_2_list = [1,2,3,4,5,6,7,8,9,None]
param_3_list = ["sqrt", "log2"]

'''
[Amit] Same as Decision tree - we recieve a grid with all possible combinations.
'''
param_grid = list(itertools.product(param_1_list, param_2_list, param_3_list))

for param_1, param_2, param_3 in param_grid:
    with mlflow.start_run():

        # Log parameters
        mlflow.log_param("model_type", "RandomForest")
        mlflow.log_param("param_1", param_1)
        mlflow.log_param("param_2", param_2)
        mlflow.log_param("param_3", param_3)

        # Train the model
        rf = RandomForestClassifier(n_estimators=param_1, max_depth=param_2, max_features=param_3)
        rf.fit(X_train, Y_train)
        pred = rf.predict(X_test)



        # run test prediction and calculate metrics:
        # accuracy_score
        # precision_score
        # recall_score
        # f1_score
        acc  = accuracy_score(Y_test, pred)
        pre = precision_score(Y_test, pred)
        rec  = recall_score(Y_test, pred)
        f1   = f1_score(Y_test, pred)


        # Log metrics
        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("precision_score", pre)
        mlflow.log_metric("recall_score", rec)
        mlflow.log_metric("f1_score", f1)

"""6.1. [Extra Step] View results from **RandomForestClassifier**


TODO: Remove this before submitting
"""

'''
here - same as 5.1 - we will see the top 10 results for RandomForest
'''

df = mlflow.search_runs(experiment_names=["trees_hyperparam"])
df_passed = df[df["status"] == "FINISHED"]
df_passed_randomForest = df_passed[df_passed["params.model_type"]== "RandomForest"]
df_top10_RandomForest = df_passed_randomForest.sort_values("metrics.f1_score", ascending=False).head(10)
df_top10_RandomForest

"""7. Train model_adaboost

- Library: sklearn.ensemble.AdaBoostClassifier
- Data: X_train, Y_train
- **Essential**: explore and optimize AdaBoostClassifier options
"""

from sklearn.ensemble import AdaBoostClassifier
'''
Same as DecisionTree and RandomForest - AdaBoost accept various args.
I chose the most common args to pick
which are -
n_estimators
learning_rate
estimator__min_samples_split
'''

param_1_list = [1, 5, 10, 50, 100, 200, 500, 1000]
param_2_list = [0.01, 0.1, 1.0]
param_3_list = [2,5,10]

param_grid = list(itertools.product(param_1_list, param_2_list, param_3_list))
for param_1, param_2, param_3 in param_grid:
    with mlflow.start_run():

        # Log parameters
        #TODO: Wait for mail from Igor if this is indeed a bug or not.
        mlflow.log_param("model_type", "AdaBoost")
        mlflow.log_param("param_1", param_1)
        mlflow.log_param("param_2", param_2)
        mlflow.log_param("param_3", param_3)
        '''
        [Amit] This is a tricky part - param3 is being used from DecisionTree, so when we parse it - we can't simply say estimator=param3.
        we need to tell AdaBoostClassifier that we are using a param from DecisionTree
        '''
        ada = AdaBoostClassifier(n_estimators=param_1, learning_rate=param_2, estimator=DecisionTreeClassifier(min_samples_split=param_3))
        ada.fit(X_train, Y_train)
        pred = ada.predict(X_test)

        # run test prediction and calculate metrics:
        # accuracy_score
        # precision_score
        # recall_score
        # f1_score
        acc  = accuracy_score(Y_test, pred)
        pre = precision_score(Y_test, pred)
        rec  = recall_score(Y_test, pred)
        f1   = f1_score(Y_test, pred)


        # Log metrics
        mlflow.log_metric("accuracy", acc)
        mlflow.log_metric("precision_score", pre)
        mlflow.log_metric("recall_score", rec)
        mlflow.log_metric("f1_score", f1)

"""7.1. [Extra Step] View results from **AdaBoostClasifier**


TODO: Remove this before submitting
"""

'''
here - same as 5.1 and 6.1 - we will see the top 10 results for AdaBoost
'''

df = mlflow.search_runs(experiment_names=["trees_hyperparam"])
df_passed = df[df["status"] == "FINISHED"]
df_passed_AdaBoost = df_passed[df_passed["params.model_type"]== "AdaBoost"]
df_top10_AdaBoost = df_passed_AdaBoost.sort_values("metrics.f1_score", ascending=False).head(10)
df_top10_AdaBoost

"""8. Store the result"""

from google.colab import files

df = df.drop(columns=[col for col in df.columns if "time" in col.lower()], errors="ignore")

df.to_excel("student_name_results.xlsx", index=False)

files.download("student_name_results.xlsx")